{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 黃柏維\n",
    "\n",
    "Student ID: 110062659\n",
    "\n",
    "GitHub ID: v54dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2021-Lab1-master Repo](https://github.com/fhcalderon87/DM2021-Lab1-master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2021-Lab1-master Repo](https://github.com/fhcalderon87/DM2021-Lab1-master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2021-Lab1-master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 4th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 1 (5 min): **  \n",
    "In this exercise, please print out the *text* data for the first three samples in the dataset. (See the above code for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in twenty_train.data[:2]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "print(X[\"text\"].sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 3 (5 min): **  \n",
    "Try to fecth records belonging to the ```comp.graphics``` category, and query every 10th record. Only show the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X.loc[lambda f: f.category_name== 'comp.graphics'].iloc[::10][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 4 (5 min):** \n",
    "Let's try something different. Instead of calculating missing values by column let's try to calculate the missing values in every record instead of every column.  \n",
    "$Hint$ : `axis` parameter. Check the documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.   \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NA_dict = [{ 'id': 'A', 'missing_example': np.nan },\n",
    "           { 'id': 'B'                    },\n",
    "           { 'id': 'C', 'missing_example': 'NaN'  },\n",
    "           { 'id': 'D', 'missing_example': 'None' },\n",
    "           { 'id': 'E', 'missing_example':  None  },\n",
    "           { 'id': 'F', 'missing_example': ''     }]\n",
    "\n",
    "NA_df = pd.DataFrame(NA_dict, columns = ['id','missing_example'])\n",
    "NA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_df['missing_example'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "'''\n",
    "isnull() only finds an array-like object. So for ['C'] and ['D'], 'NaN' and 'None' are consider an array.\n",
    "np.nan and ['E'] None are null. ['F'] has a array but with empty element.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes to the `X` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X[0:10]\n",
    " \n",
    "# no, there is no changes to X dataframe. Only X_sample changes for every execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 7 (5 min):**\n",
    "Notice that for the `ylim` parameters we hardcoded the maximum value for y. Is it possible to automate this instead of hard-coding it? How would you go about doing that? (Hint: look at code above for clues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "upper_bound = max(X_sample.category_name.value_counts())*1.1\n",
    "print(X_sample.category_name.value_counts())\n",
    "\n",
    "# plot barchart for X_sample\n",
    "X_sample.category_name.value_counts().plot(kind = 'bar',\n",
    "                                          title = 'Category distribution',\n",
    "                                          ylim = [0, upper_bound],\n",
    "                                          rot = 0,\n",
    "                                          fontsize = 12,\n",
    "                                          figsize = (8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 8 (take home):** \n",
    "We can also do a side-by-side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Below we show you an snapshot of the type of chart we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "x_pos = np.arange(4)\n",
    "plt.bar(x_pos-0.1,X.category_name.value_counts(),0.2)\n",
    "plt.bar(x_pos+0.1,X_sample.category_name.value_counts(),0.2)\n",
    "\n",
    "#plt.bar(x_pos,X_sample.category_name.value_counts(),2)\n",
    "#plt.bar(x_pos + 1,X.category_name.value_counts(),2)\n",
    "\n",
    "plt.xticks(x_pos,(\"soc.religion.christian\",\"sci.med\",\"comp.graphics\",\"alt.atheism\"))\n",
    "plt.title(\"category distribution\")\n",
    "plt.legend([\"category_name\",\"category_name\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 9 (5 min):**\n",
    "Let's analyze the first record of our X dataframe with the new analyzer we have just built. Go ahead try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "analyze(\" \".join(list(X[:1].text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 10 (take home):**\n",
    "We said that the `1` at the beginning of the fifth record represents the `00` term. Notice that there is another 1 in the same record. Can you provide code that can verify what word this 1 represents from the vocabulary. Try to do this as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "temp = X_counts[4,0:100].toarray()\n",
    "\n",
    "for i in range(100):\n",
    "    if temp[0][i] == 1:\n",
    "        print(count_vect.get_feature_names()[0:100][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 11 (take home):** \n",
    "From the chart above, we can see how sparse the term-document matrix is; i.e., there is only one terms with frequency of `1` in the subselection of the matrix. By the way, you may have noticed that we only selected 20 articles and 20 terms to plot the histrogram. As an excersise you can try to modify the code above to plot the entire term-document matrix or just a sample of it. How would you do this efficiently? Remember there is a lot of words in the vocab. Report below what methods you would use to get a nice and useful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X.text)\n",
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]\n",
    "temp = [i for i in range(X_counts.shape[1])]\n",
    "temp.sort(key = lambda s: term_frequencies[s], reverse = True)\n",
    "X_top20 = temp[:20]\n",
    "\n",
    "plot_x = [\"term_\" + count_vect.get_feature_names()[i] for i in X_top20]\n",
    "plot_y = [\"doc_\" + str(i) for i in list (X.index)[0:20]]\n",
    "plot_z = X_counts[0:20,tuple(X_top20)].toarray()\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z,columns = plot_x,index=plot_y)\n",
    "plt.subplots(figsize=(9,7))\n",
    "ax= sns.heatmap(df_todraw, cmap = \"PuRd\",vmin =0,vmax = 20,annot= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 12 (take home):\n",
    "Please try to reduce the dimension to 3, and plot the result use 3-D plot. Use at least 3 different angle (camera position) to check your result and describe what you found.\n",
    "\n",
    "$Hint$: you can refer to Axes3D in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X_reduced = PCA(n_components = 3).fit_transform(X_counts.toarray())\n",
    "plt.figure(figsize=(30,25))\n",
    "\n",
    "ax1 = plt.subplot(131,projection = '3d')\n",
    "ax2 = plt.subplot(132,projection = '3d')\n",
    "ax3 = plt.subplot(133,projection = '3d')\n",
    "\n",
    "\n",
    "for c ,category in zip(col,categories):\n",
    "    xs = X_reduced[X['category_name'] == category].T[0]\n",
    "    ys = X_reduced[X['category_name'] == category].T[1]\n",
    "    zs = X_reduced[X['category_name'] == category].T[2]\n",
    "    ax1.scatter(xs, ys, zs, c=c,marker='o')\n",
    "    ax2.scatter(xs, ys, zs, c=c,marker='o')\n",
    "    ax3.scatter(xs, ys, zs, c=c,marker='o')\n",
    "    \n",
    "ax1.view_init(25,-45)\n",
    "ax2.view_init(25,0)\n",
    "ax3.view_init(25,45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 13 (take home):**\n",
    "If you want a nicer interactive visualization here, I would encourage you try to install and use plotly to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "x_selected = []\n",
    "y_selected = []\n",
    "\n",
    "for i in range(len(count_vect.get_feature_names())):\n",
    "    if term_frequencies[i] > 100:\n",
    "        x_selected.append(count_vect.get_feature_names()[i])\n",
    "        y_selected.append(count_vect.get_feature_names()[i])\n",
    "\n",
    "fig = go.Figure(\n",
    "    data  = [go.Bar(x=x_selected, y = y_selected)]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 14 (take home):** \n",
    "The chart above contains all the vocabulary, and it's computationally intensive to both compute and visualize. Can you efficiently reduce the number of terms you want to visualize as an exercise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "plt.subplots(figsize=(100, 10))\n",
    "\n",
    "x_selected = []\n",
    "y_selected = []\n",
    "\n",
    "for i in range(len(count_vect.get_feature_names())):\n",
    "    if term_frequencies[i] > 500:\n",
    "        x_selected.append(count_vect.get_feature_names()[i])\n",
    "        y_selected.append(term_frequencies[i])\n",
    "            \n",
    "\n",
    "g = sns.barplot(x=x_selected,y = y_selected)\n",
    "g.set_xticklabels(x_selected, rotation = 90);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 15 (take home):** \n",
    "Additionally, you can attempt to sort the terms on the `x-axis` by frequency instead of in alphabetical order. This way the visualization is more meaninfgul and you will be able to observe the so called [long tail](https://en.wikipedia.org/wiki/Long_tail) (get familiar with this term since it will appear a lot in data mining and other statistics courses). see picture below\n",
    "\n",
    "![alt txt](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1000px-Long_tail.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "\n",
    "long_tail_distribution = np.sort(term_frequencies)[::-1]\n",
    "\n",
    "x = []\n",
    "#x = [i+1 for i in range(len(long_tail_distribution[:1000]))]\n",
    "for i in range(len(long_tail_distribution[:500])):\n",
    "    x.append(i+1)\n",
    "y = long_tail_distribution[:500]\n",
    "\n",
    "fig =plt.figure(figsize = (25,10))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "#print(x)\n",
    "#print(y)\n",
    "ax.plot(x,y,linewidth=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 16 (take home):**\n",
    "Try to generate the binarization using the `category_name` column instead. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(X.category_name)\n",
    "X['bin_category_name'] = mlb.transform(X['category_name']).tolist()\n",
    "X[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
